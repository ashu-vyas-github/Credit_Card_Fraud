{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working list\n",
    "\n",
    "6. Add explanations and analysis\n",
    "\n",
    "~asda~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the main code file to be implemented as both .py and .ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Importing relevant packages and libraries\n",
    "\n",
    "# graph plotting, maths, execution libraries\n",
    "import os, matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# feature scaling, selection; over-sampling; scoring metrics libraries\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit, learning_curve\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score, precision_score, recall_score, plot_confusion_matrix\n",
    "\n",
    "# linear and non-linear supervised learning classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# unsupervised learning classifiers\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports Done!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the data\n",
    "\n",
    "# If using kaggle, then enable this block of code for input file\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         data_file = os.path.join(dirname, filename)\n",
    "#         print(data_file)\n",
    "\n",
    "# If using a directory on own computer, enable this block of code\n",
    "data_path = r'/media/ashutosh/Computer Vision/Predictive_Maintenance/Bank_Loan_data_Kaggle' # for Linux systems\n",
    "#data_path = r'E:\\Predictive_Maintenance\\Bank_Loan_data_Kaggle' # for Windows systems\n",
    "data_file = data_path+\"//creditcard.csv\"\n",
    "\n",
    "# load the data file from the specified path\n",
    "data_csv = pd.read_csv(data_file)\n",
    "print(data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 functions definition: data preprocessing, plotting learning curves, Autoencoder model, scoring metrics, classifier fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_minmax_outliers_helper(data_loaded, features_list):\n",
    "    \n",
    "    data_csv = data_loaded\n",
    "    data_csv_fraud = data_csv[data_csv['Class'] == 1]\n",
    "    min_fraud = data_csv_fraud[features_list].min(axis=0)\n",
    "    max_fraud = data_csv_fraud[features_list].max(axis=0)\n",
    "    for onefeature in features_list:\n",
    "        if onefeature in ['Class']:\n",
    "            continue\n",
    "        else:\n",
    "            data_csv = data_csv[data_csv[onefeature] >= min_fraud[onefeature]]\n",
    "            data_csv = data_csv[data_csv[onefeature] <= max_fraud[onefeature]]\n",
    "    \n",
    "    return data_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_over_sampling_helper(normal_sampled, fraud_sampled):\n",
    "    \n",
    "    smote_over_sample = BorderlineSMOTE(sampling_strategy='minority', random_state=42, k_neighbors=5, n_jobs=-1, m_neighbors=10, kind='borderline-1')\n",
    "    train_concat = pd.concat([normal_sampled, fraud_sampled], axis=0)\n",
    "    train_classes = train_concat['Class']\n",
    "    train_concat = train_concat.drop(['Class'], axis=1)\n",
    "    X_resampled, y_resampled = smote_over_sample.fit_resample(train_concat, train_classes)\n",
    "    train_concat = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "    fraud_sampled = train_concat[train_concat['Class'] == 1]\n",
    "    normal_sampled = train_concat[train_concat['Class'] == 0]\n",
    "\n",
    "    return normal_sampled, fraud_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_feature_scaling(X_train, X_valid, features_numerical):\n",
    "    \n",
    "    stdscl = StandardScaler()\n",
    "    train_num = np.asarray(X_train[features_numerical])\n",
    "    valid_num = np.asarray(X_valid[features_numerical])\n",
    "    X_train_num_ss = stdscl.fit_transform(train_num.reshape(-1,1))\n",
    "    X_valid_num_ss = stdscl.transform(valid_num.reshape(-1,1))\n",
    "    X_train = X_train.drop(features_numerical, axis=1)\n",
    "    X_valid = X_valid.drop(features_numerical, axis=1)\n",
    "    X_train[features_numerical] = X_train_num_ss\n",
    "    X_valid[features_numerical] = X_valid_num_ss\n",
    "        \n",
    "    return X_train, X_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_data_process(data_csv, normal_sampled, fraud_sampled):\n",
    "    \n",
    "    X_train = pd.concat([normal_sampled, fraud_sampled], axis=0)\n",
    "    y_train = X_train['Class']\n",
    "    X_train = X_train.drop(['Class'], axis=1)\n",
    "    y_valid = data_csv['Class']\n",
    "    data_csv = data_csv.drop(['Class'], axis=1)\n",
    "    X_valid = data_csv\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_data_process(data_csv, normal_sampled, fraud_sampled, unsupervised_train_onfraud=False):\n",
    "    \n",
    "    if unsupervised_train_onfraud in [False]: #Training unsupervised model on normal transactions samples\n",
    "        X_train = normal_sampled\n",
    "        y_train = X_train['Class']\n",
    "        X_train = X_train.drop(['Class'], axis=1)\n",
    "        X_valid = pd.concat([data_csv, fraud_sampled], axis=0)\n",
    "        y_valid = X_valid['Class']\n",
    "        X_valid = X_valid.drop(['Class'], axis=1)\n",
    "\n",
    "    elif unsupervised_train_onfraud in [True]: #Training unsupervised model on fraudulent transactions samples\n",
    "        X_train = fraud_sampled\n",
    "        y_train = X_train['Class']\n",
    "        X_train = X_train.drop(['Class'], axis=1)\n",
    "        X_valid = pd.concat([data_csv, normal_sampled], axis=0)\n",
    "        y_valid = X_valid['Class']\n",
    "        X_valid = X_valid.drop(['Class'], axis=1)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semisupervised_data_process(data_csv, normal, fraud, normal_sampled, fraud_sampled, unsupervised_train_onfraud=False):\n",
    "\n",
    "    if unsupervised_train_onfraud in [False]: #Training unsupervised model on normal transactions samples\n",
    "        X_train = normal #normal_sampled\n",
    "        y_train = X_train['Class']\n",
    "        X_train = X_train.drop(['Class'], axis=1)\n",
    "        X_valid = fraud_sampled #pd.concat([data_csv, fraud_sampled], axis=0)\n",
    "        y_valid = X_valid['Class']\n",
    "        X_valid = X_valid.drop(['Class'], axis=1)\n",
    "\n",
    "    elif unsupervised_train_onfraud in [True]: #Training unsupervised model on fraudulent transactions samples\n",
    "        X_train = fraud_sampled\n",
    "        y_train = X_train['Class']\n",
    "        X_train = X_train.drop(['Class'], axis=1)\n",
    "        X_valid = normal #normal_sampled #pd.concat([data_csv, normal_sampled], axis=0)\n",
    "        y_valid = X_valid['Class']\n",
    "        X_valid = X_valid.drop(['Class'], axis=1)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_supervised_classifier(X_normal, X_fraud):\n",
    "\n",
    "    ## input layer\n",
    "    input_layer = Input(shape=(X_normal.shape[1],))\n",
    "\n",
    "    ## encoding part\n",
    "    encoded = Dense(200, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "    encoded = Dense(50, activation='relu')(encoded)\n",
    "    #encoded = Dense(25, activation='relu')(encoded)\n",
    "\n",
    "    ## decoding part\n",
    "    #decoded = Dense(25, activation='tanh')(encoded)\n",
    "    decoded = Dense(50, activation='tanh')(encoded)\n",
    "    decoded = Dense(200, activation='tanh')(decoded)\n",
    "\n",
    "    ## output layer\n",
    "    output_layer = Dense(X_normal.shape[1], activation='relu')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n",
    "    autoencoder.fit(X_normal, X_normal, batch_size = 256, epochs = 10, shuffle = True, validation_split = 0.20, verbose=0)\n",
    "\n",
    "    hidden_representation = Sequential()\n",
    "    hidden_representation.add(autoencoder.layers[0])\n",
    "    hidden_representation.add(autoencoder.layers[1])\n",
    "    hidden_representation.add(autoencoder.layers[2])\n",
    "    norm_hid_rep = hidden_representation.predict(X_normal)\n",
    "    fraud_hid_rep = hidden_representation.predict(X_fraud)\n",
    "\n",
    "    X_represent_transformed = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\n",
    "    y_normal = np.zeros(norm_hid_rep.shape[0])\n",
    "    y_frauds = np.ones(fraud_hid_rep.shape[0])\n",
    "    y_represent_transformed = np.append(y_normal, y_frauds)\n",
    "\n",
    "    return X_represent_transformed, y_represent_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_recursive_elimination_helper(estimator, xtrn, xvld, ytrn, steps, cvld, score_met):\n",
    "    \n",
    "    rfecv = RFECV(estimator, step=steps, min_features_to_select=1, cv=cvld, scoring=score_met, verbose=0, n_jobs=-1)\n",
    "    try:\n",
    "        X_train_new = rfecv.fit_transform(xtrn, ytrn)\n",
    "    except RuntimeError:\n",
    "        logreg_rf = LogisticRegression(C=1.0, penalty='l2', dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=42, solver='lbfgs', max_iter=1000, multi_class='auto', verbose=0, warm_start=False, n_jobs=-1, l1_ratio=None)\n",
    "        rfecv = RFECV(logreg_rf, step=steps, min_features_to_select=1, cv=cvld, scoring=score_met, verbose=0, n_jobs=-1)\n",
    "        X_train_new = rfecv.fit_transform(xtrn, ytrn)\n",
    "    print(\"Optimal features: %d\" % rfecv.n_features_)\n",
    "    X_valid_new = rfecv.transform(xvld)\n",
    "    xtrn = X_train_new\n",
    "    xvld = X_valid_new\n",
    "    \n",
    "    return xtrn, xvld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gscv_helper(estimator, xtrn, xvld, ytrn, param_grid, cvld, score_met):\n",
    "    \n",
    "    print(param_grid)\n",
    "    gscv = GridSearchCV(estimator, param_grid=param_grid, scoring=score_met, n_jobs=-1, refit=True, cv=cvld, verbose=0, pre_dispatch='2*n_jobs', return_train_score=False)\n",
    "    gscv.fit(xtrn, ytrn)\n",
    "    ytrn_pred = gscv.predict(xtrn)\n",
    "    yvld_pred = gscv.predict(xvld)\n",
    "    ytrn_pred_proba_both = gscv.predict_proba(xtrn)\n",
    "    ytrn_pred_proba = ytrn_pred_proba_both[:,1]\n",
    "    yvld_pred_proba_both = gscv.predict_proba(xvld)\n",
    "    yvld_pred_proba = yvld_pred_proba_both[:,1]\n",
    "    print(gscv.best_estimator_)\n",
    "    print(gscv.best_score_)\n",
    "    print(gscv.best_params_)\n",
    "    estimator = gscv.best_estimator_\n",
    "    \n",
    "    return estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_training(estimator, xtrn, xvld, ytrn):\n",
    "    \n",
    "    estimator.fit(xtrn, ytrn)\n",
    "    ytrn_pred = estimator.predict(xtrn)\n",
    "    yvld_pred = estimator.predict(xvld)\n",
    "    ytrn_pred_proba_both = estimator.predict_proba(xtrn)\n",
    "    ytrn_pred_proba = ytrn_pred_proba_both[:,1]\n",
    "    yvld_pred_proba_both = estimator.predict_proba(xvld)\n",
    "    yvld_pred_proba = yvld_pred_proba_both[:,1]\n",
    "    \n",
    "    return estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_learning(estimator, xtrn, xvld):\n",
    "    \n",
    "    estimator.fit(xtrn)\n",
    "    ytrn_pred = estimator.predict(xtrn)\n",
    "    yvld_pred = estimator.predict(xvld)\n",
    "    ytrn_pred = [1 if l == -1 else 0 for l in ytrn_pred]\n",
    "    yvld_pred = [1 if l == -1 else 0 for l in yvld_pred]\n",
    "    ytrn_pred_proba = ytrn_pred\n",
    "    yvld_pred_proba = yvld_pred\n",
    "    \n",
    "    return estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semisupervised_learning(estimator, xtrn, xvld):\n",
    "    \n",
    "    X_represent_transformed, y_represent_transformed = autoencoder_supervised_classifier(xtrn, xvld)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_represent_transformed, y_represent_transformed, test_size=0.25)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    ytrn_pred = estimator.predict(X_train)\n",
    "    yvld_pred = estimator.predict(X_valid)\n",
    "    ytrn_pred_proba_both = estimator.predict_proba(X_train)\n",
    "    ytrn_pred_proba = ytrn_pred_proba_both[:,1]\n",
    "    yvld_pred_proba_both = estimator.predict_proba(X_valid)\n",
    "    yvld_pred_proba = yvld_pred_proba_both[:,1]\n",
    "    ytrn = y_train\n",
    "    yvld = y_valid\n",
    "\n",
    "    return estimator, ytrn, yvld, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_metrics_calculation(ytrn_true, yvld_true, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba, return_train_score=False):\n",
    "\n",
    "    if return_train_score == True:\n",
    "        accur = accuracy_score(ytrn_true,ytrn_pred)\n",
    "        precs = precision_score(ytrn_true, ytrn_pred, average='weighted')\n",
    "        recal = recall_score(ytrn_true, ytrn_pred, average='weighted')\n",
    "        auprc = average_precision_score(ytrn_true,ytrn_pred_proba)\n",
    "        conmat = confusion_matrix(ytrn_true, ytrn_pred, normalize='all')\n",
    "        tn, fp, fn, tp = conmat.ravel()\n",
    "\n",
    "        score_dict_train = dict(accur=accur, precs=precs, recal=recal, auprc=auprc, tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "    else:\n",
    "        score_dict_train = 0\n",
    "\n",
    "    accur = accuracy_score(yvld_true,yvld_pred)\n",
    "    precs = precision_score(yvld_true, yvld_pred, average='weighted')\n",
    "    recal = recall_score(yvld_true, yvld_pred, average='weighted')\n",
    "    auprc = average_precision_score(yvld_true,yvld_pred_proba)\n",
    "    conmat = confusion_matrix(yvld_true, yvld_pred, normalize='true')\n",
    "    tn, fp, fn, tp = conmat.ravel()\n",
    "\n",
    "    score_dict_valid = dict(accur=accur, precs=precs, recal=recal, auprc=auprc, tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "\n",
    "    return score_dict_train, score_dict_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, score_met='average_precision', ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 7)):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, dpi=dpi_setting)\n",
    "\n",
    "    axes.set_title(\"Learning Curve for \"+title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(score_met)\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, scoring=score_met, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, return_times=False)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    axes.legend(loc=\"best\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data_loaded, features_drop_list=['Time'], ml_method='Supervised', fraction_sample=0.8, multiplier_factor_sample=1, unsupervised_train_onfraud=False, remove_minmax_outliers=False, do_over_sampling=False):\n",
    "\n",
    "    data_csv = data_loaded\n",
    "\n",
    "    try:\n",
    "        data_csv = data_csv.drop(features_drop_list, axis=1)\n",
    "        features_list = list(data_csv.columns)\n",
    "    except KeyError:\n",
    "        features_list = list(data_csv.columns)            \n",
    "\n",
    "    if remove_minmax_outliers in [True]:\n",
    "        data_csv = remove_minmax_outliers_helper(data_csv, features_list)\n",
    "    \n",
    "    fraud = data_csv[data_csv['Class'] == 1]\n",
    "    normal = data_csv[data_csv['Class'] == 0]\n",
    "    fraud_sampled = fraud.sample(frac=fraction_sample)\n",
    "    normal_sampled = normal.sample(int(multiplier_factor_sample*fraud_sampled.shape[0]))\n",
    "    data_csv = data_csv.drop(fraud_sampled.index)\n",
    "    data_csv = data_csv.drop(normal_sampled.index)\n",
    "    data_csv = data_csv.reset_index(drop=True)\n",
    "\n",
    "    if do_over_sampling in [True]:\n",
    "        normal_sampled, fraud_sampled = do_over_sampling_helper(normal_sampled, fraud_sampled)\n",
    "    \n",
    "    if ml_method in ['Supervised']:\n",
    "        X_train, X_valid, y_train, y_valid = supervised_data_process(data_csv, normal_sampled, fraud_sampled)\n",
    "\n",
    "    elif ml_method in ['Unsupervised']:\n",
    "        X_train, X_valid, y_train, y_valid = unsupervised_data_process(data_csv, normal_sampled, fraud_sampled, unsupervised_train_onfraud=unsupervised_train_onfraud)\n",
    "\n",
    "    elif ml_method in ['Semisupervised']:\n",
    "        X_train, X_valid, y_train, y_valid = semisupervised_data_process(data_csv, normal, fraud, normal_sampled, fraud_sampled, unsupervised_train_onfraud=unsupervised_train_onfraud)\n",
    "\n",
    "    # Standard scaling feature 'Amount'\n",
    "    features_numerical = 'Amount'\n",
    "    if features_numerical in features_list:\n",
    "        X_train, X_valid = numerical_feature_scaling(X_train, X_valid, features_numerical)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning_model(estimator, xtrn, xvld, ytrn, yvld, ml_method='Supervised', n_splits=3, steps=1, score_met='average_precision', param_grid=None, do_gscv=False, do_recursive_elimination=False, plot_learn_curve=False, plot_con_matrix=False, return_train_score=False):\n",
    "\n",
    "    cvld = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, train_size=None, random_state=42)\n",
    "\n",
    "    if do_recursive_elimination in [True]:\n",
    "        # estimator.fit(xtrn, ytrn)\n",
    "        print(\"\\nRecursive Feature Eliminiation.....\")\n",
    "        xtrn, xvld = do_recursive_elimination_helper(estimator, xtrn, xvld, ytrn, steps, cvld, score_met)\n",
    "\n",
    "    if do_gscv in [True]:\n",
    "        print(\"\\nGrid Search CV.....\")\n",
    "        estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba = do_gscv_helper(estimator, xtrn, xvld, ytrn, param_grid, cvld, score_met)\n",
    "        \n",
    "    if ml_method in ['Supervised']:\n",
    "        estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba = supervised_training(estimator, xtrn, xvld, ytrn)\n",
    "\n",
    "    elif ml_method in ['Unsupervised']:\n",
    "        estimator, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba = unsupervised_learning(estimator, xtrn, xvld)\n",
    "        \n",
    "    elif ml_method in ['Semisupervised']:\n",
    "        estimator, ytrn, yvld, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba = semisupervised_learning(estimator, xtrn, xvld)\n",
    "        \n",
    "    score_dict_train, score_dict_valid = scoring_metrics_calculation(ytrn, yvld, ytrn_pred, yvld_pred, ytrn_pred_proba, yvld_pred_proba, return_train_score=return_train_score)\n",
    "\n",
    "    if plot_learn_curve in [True]:\n",
    "        print(\"\\nPlotting Learning Curve.....\")\n",
    "        title = str(estimator)\n",
    "        plot_learning_curve(estimator, title, xtrn, ytrn, ylim=(0.0, 1.1), cv=cvld, n_jobs=-1)\n",
    "        plt.show()\n",
    "\n",
    "    if plot_con_matrix in [True]:\n",
    "        print(\"Plotting Confusion Matrix.....\")\n",
    "        plot_confusion_matrix(estimator, xvld, yvld, labels=None, sample_weight=None, normalize='true', display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None)\n",
    "        plt.show()\n",
    "\n",
    "    return score_dict_train, score_dict_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12 classifiers initialization\n",
    "\n",
    "Log_Reg = LogisticRegression(C=1.0, penalty='l2', dual=False, tol=0.0001, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=42, solver='lbfgs', max_iter=1000, multi_class='auto', verbose=0, warm_start=False, n_jobs=-1, l1_ratio=None)\n",
    "\n",
    "SVC_Linear = SVC(C=1.0, kernel='linear', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovo', break_ties=False, random_state=42)\n",
    "\n",
    "SVC_RBF = SVC(C=100.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=True, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovo', break_ties=False, random_state=42)\n",
    "\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth=16, min_samples_split=26, min_samples_leaf=2, max_features=None, random_state=42, class_weight=None)\n",
    "\n",
    "Random_Forest = RandomForestClassifier(n_estimators=90, criterion='entropy', max_depth=10, min_samples_split=8, min_samples_leaf=2, max_features='sqrt', bootstrap=True, oob_score=False, class_weight=None, n_jobs=-1, random_state=42)\n",
    "\n",
    "XGBoost = XGBClassifier(n_estimators=100, max_depth=5, min_child_weight=2, max_delta_step=8, learning_rate=0.1, gamma=0.1, objective='binary:logistic', scale_pos_weight=1, base_score=0.85, missing=None, n_jobs=-1, nthread=-1, random_state=42, seed=42, silent=True, subsample=1, verbosity=0)\n",
    "\n",
    "LightGBM = LGBMClassifier(n_estimators=115, num_leaves=65, max_depth=15, min_child_samples=40, learning_rate=0.1, boosting_type='gbdt', objective='binary', random_state=42, n_jobs=- 1, silent=True)\n",
    "\n",
    "Naive_Bayes = GaussianNB(var_smoothing=1e0)\n",
    "\n",
    "One_Class_SVM = OneClassSVM(kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, nu=0.05, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "\n",
    "Isolation_Forest = IsolationForest(n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "Auto_Enc_LogReg = Log_Reg\n",
    "\n",
    "Auto_Enc_LightGBM = LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables initializations\n",
    "dpi_setting=120\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plot_linewidth = 1.5\n",
    "startTime= datetime.now()\n",
    "\n",
    "no_features = ['V5','V6','V8','V13','V15','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "maybe_relevant_features = ['V7','V1','V2','V3','V16','V18']\n",
    "\n",
    "score_met = 'average_precision'\n",
    "fraction_sample = 0.2\n",
    "multiplier_factor_sample = 20\n",
    "n_splits = 5\n",
    "steps = 1\n",
    "unsupervised_train_onfraud = False\n",
    "do_gscv = False\n",
    "do_over_sampling = True\n",
    "do_recursive_elimination = False\n",
    "plot_learn_curve = False\n",
    "plot_con_matrix = False\n",
    "param_grid = None\n",
    "\n",
    "remove_features = ['Time'] # + no_features #+ maybe_relevant_features\n",
    "remove_minmax_outliers = False\n",
    "return_train_score = False\n",
    "\n",
    "\n",
    "classifiers_text_list = ['Log_Reg', 'SVC_Linear', 'SVC_RBF', 'Decision_Tree', 'Random_Forest', 'XGBoost', 'LightGBM', 'Naive_Bayes', 'One_Class_SVM', 'Isolation_Forest', 'Auto_Enc_LogReg', 'Auto_Enc_LightGBM']\n",
    "ml_methods_list = ['Supervised', 'Supervised', 'Supervised', 'Supervised', 'Supervised', 'Supervised', 'Supervised', 'Supervised', 'Unsupervised', 'Unsupervised', 'Semisupervised', 'Semisupervised']\n",
    "classifiers_list = [Log_Reg, SVC_Linear, SVC_RBF, Decision_Tree, Random_Forest, XGBoost, LightGBM, Naive_Bayes, One_Class_SVM, Isolation_Forest, Auto_Enc_LogReg, Auto_Enc_LightGBM]\n",
    "metrics_list = ['accur', 'precs', 'recal', 'auprc', 'tn', 'fp', 'fn', 'tp']\n",
    "seed_vals_list = [13, 77, 42, 639, 41]\n",
    "runs_list = ['run'+str(x) for x in seed_vals_list]\n",
    "\n",
    "pd_df_columns = pd.MultiIndex.from_product([classifiers_text_list, metrics_list, runs_list], names=['classifiers', 'metrics', 'runs'])\n",
    "zeros_init = np.zeros(pd_df_columns.shape[0])\n",
    "all_clfs_valid_scores = pd.DataFrame(0.0, index=[0], columns=pd_df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_features_data_analysis(data_csv):\n",
    "\n",
    "    data_csv = data_csv.drop(['Time'],axis=1)\n",
    "    features_list = list(data_csv.columns)\n",
    "    # data_csv = data_csv.drop(['Class'],axis=1)\n",
    "    sample_nums = data_csv.index\n",
    "    label_color= ['green' if l == 0 else 'red' for l in data_csv['Class']]\n",
    "    data_csv_fraud = data_csv[data_csv['Class'] == 1]\n",
    "    data_csv_normal = data_csv[data_csv['Class'] == 0]\n",
    "    mean_fraud = data_csv_fraud[features_list].mean(axis=0)\n",
    "    mean_normal = data_csv_normal[features_list].mean(axis=0)\n",
    "    std_fraud = data_csv_fraud[features_list].std(axis=0)\n",
    "    std_normal = data_csv_normal[features_list].std(axis=0)\n",
    "    min_fraud = data_csv_fraud[features_list].min(axis=0)\n",
    "    max_fraud = data_csv_fraud[features_list].max(axis=0)\n",
    "    sample_nums_fraud = [x for x in range(data_csv_fraud.shape[0])]\n",
    "    \n",
    "    #### Plotting individual feature to understand outliers\n",
    "    for onefeature in features_list:\n",
    "        plot_df = data_csv[onefeature]\n",
    "        plt.figure(num=None, figsize=(4, 3), dpi=dpi_setting, facecolor='w', edgecolor='w')\n",
    "        plt.title(onefeature+\" Scatter plot\")\n",
    "        plt.xlim(0,300000)\n",
    "        plt.xlabel('Sample no.')\n",
    "        plt.ylabel(onefeature)\n",
    "        plt.scatter(sample_nums, plot_df, color=label_color, s=6)\n",
    "        plt.axhline(y=mean_fraud[onefeature], xmin=0, xmax=1, color='r', linestyle='-',linewidth=plot_linewidth, marker='o')\n",
    "        plt.axhline(y=mean_normal[onefeature], xmin=0, xmax=1, color='g', linestyle='-',linewidth=plot_linewidth, marker='o')\n",
    "        plt.grid(b=True, which='major', axis='both', linestyle=':', linewidth=0.5, alpha=1)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "plotting_features_data_analysis(data_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n.......................................................     Start     .......................................................\\n\")\n",
    "\n",
    "for one_clf, one_clf_text, one_ml_method in zip(classifiers_list, classifiers_text_list, ml_methods_list):\n",
    "    print(one_clf_text)\n",
    "    if one_ml_method in ['Semisupervised']:\n",
    "        multiplier_factor_sample = 250\n",
    "    else:\n",
    "        multiplier_factor_sample = 20\n",
    "\n",
    "    for one_seed, one_run in zip(seed_vals_list, runs_list):\n",
    "        np.random.seed(one_seed)\n",
    "        ml_method = one_ml_method\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = data_preprocessing(data_csv, features_drop_list=remove_features, ml_method=ml_method, unsupervised_train_onfraud=unsupervised_train_onfraud, remove_minmax_outliers=remove_minmax_outliers, fraction_sample=fraction_sample, multiplier_factor_sample=multiplier_factor_sample, do_over_sampling=do_over_sampling)\n",
    "\n",
    "        score_dict_train, score_dict_valid = machine_learning_model(one_clf, X_train, X_valid, y_train, y_valid, ml_method=ml_method, n_splits=n_splits, steps=steps, score_met=score_met, param_grid=param_grid, do_gscv=do_gscv, do_recursive_elimination=do_recursive_elimination, plot_learn_curve=plot_learn_curve, plot_con_matrix=plot_con_matrix, return_train_score=return_train_score)\n",
    "\n",
    "        for one_metric in metrics_list:\n",
    "            all_clfs_valid_scores[one_clf_text, one_metric, one_run][0] = score_dict_valid[one_metric]\n",
    "\n",
    "\n",
    "for one_metric in metrics_list:\n",
    "    print(one_metric)\n",
    "    means_list = []\n",
    "    stds_list = []\n",
    "    for one_clf_text in classifiers_text_list:\n",
    "        mean_calc = all_clfs_valid_scores[one_clf_text, one_metric].mean(axis=1)\n",
    "        means_list.append(100*mean_calc[0])\n",
    "        std_calc = all_clfs_valid_scores[one_clf_text, one_metric].std(axis=1)\n",
    "        stds_list.append(100*std_calc[0])\n",
    "\n",
    "    plt.figure(num=None, figsize=None, dpi=dpi_setting, facecolor='w', edgecolor='w')\n",
    "    plt.title(one_metric+' [2]')\n",
    "    plt.xlim()\n",
    "    plt.ylim(0,100)\n",
    "    plt.xlabel('Estimators')\n",
    "    plt.ylabel(one_metric)\n",
    "    plt.bar(x=np.arange(len(classifiers_text_list)), height=means_list, width=0.3, yerr=stds_list)\n",
    "    plt.xticks(ticks=np.arange(len(classifiers_text_list)), labels=classifiers_text_list, rotation=90)\n",
    "    plt.grid(b=True, which='major', axis='both', linestyle=':', linewidth=0.5, alpha=1)\n",
    "    # plt.savefig(data_path+\"//{txt}_removed_outliers_allfeatures.png\".format(txt=one_metric), dpi=dpi_setting, facecolor='w', edgecolor='w', orientation='portrait', papertype=None, format='png', transparent=False, bbox_inches='tight', pad_inches=0.1, metadata=None)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "print(\"\\n.......................................................      Done     .......................................................\\n\")\n",
    "timeElapsed = datetime.now() - startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Block for one classifier\n",
    "# classifiers_list: Log_Reg, SVC_Linear, SVC_RBF, Decision_Tree, Random_Forest, \n",
    "#                   XGBoost, LightGBM, Naive_Bayes, One_Class_SVM, Isolation_Forest,\n",
    "#                   Auto_Enc_LogReg, Auto_Enc_LightGBM\n",
    "\n",
    "np.random.seed(42)\n",
    "startTime= datetime.now()\n",
    "\n",
    "no_features = ['V5','V6','V8','V13','V15','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "maybe_relevant_features = ['V7','V1','V2','V3','V16','V18']\n",
    "\n",
    "score_met = 'average_precision'\n",
    "fraction_sample = 0.2\n",
    "multiplier_factor_sample = 20\n",
    "n_splits = 5\n",
    "steps = 1\n",
    "unsupervised_train_onfraud = False\n",
    "return_train_score = False\n",
    "do_gscv = False\n",
    "do_recursive_elimination = False\n",
    "plot_learn_curve = True\n",
    "plot_con_matrix = True\n",
    "param_grid = None\n",
    "\n",
    "remove_minmax_outliers = True # False\n",
    "do_over_sampling = True # False\n",
    "\n",
    "remove_features = ['Time'] + no_features #+ maybe_relevant_features\n",
    "\n",
    "ml_method = 'Supervised' # 'Unsupervised', 'Semisupervised'\n",
    "\n",
    "one_clf = Log_Reg\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = data_preprocessing(data_csv, features_drop_list=remove_features, ml_method=ml_method, unsupervised_train_onfraud=unsupervised_train_onfraud, remove_minmax_outliers=remove_minmax_outliers, fraction_sample=fraction_sample, multiplier_factor_sample=multiplier_factor_sample, do_over_sampling=do_over_sampling)\n",
    "\n",
    "score_dict_train, score_dict_valid = machine_learning_model(one_clf, X_train, X_valid, y_train, y_valid, ml_method=ml_method, n_splits=n_splits, steps=steps, score_met=score_met, param_grid=param_grid, do_gscv=do_gscv, do_recursive_elimination=do_recursive_elimination, plot_learn_curve=plot_learn_curve, plot_con_matrix=plot_con_matrix, return_train_score=return_train_score)\n",
    "\n",
    "print(\"Training Score:\")\n",
    "print(score_dict_train)\n",
    "print(\"\\nTesting Score:\")\n",
    "print(score_dict_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
